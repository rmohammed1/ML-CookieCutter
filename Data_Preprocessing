'''Quick Data Preprocessing
scikit-learn Preprocessing library'''

'''Import libraries'''
# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

'''Import dataset
Importing dataset and locating dependent and independent variables'''
# Importing the dataset 
dataset = pd.read_csv('Data.csv')
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values
print(X)
print(y)

'''Missing data
We are using mean here to use simple imputer from scikit learn library to replace the null or nan values, instead of using all, we need to specify the columns'''
# Taking care of missing data
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(X[:, 1:3])
X[:, 1:3] = imputer.transform(X[:, 1:3])
print(X)

'''Categorical data
We cannot keep text only as machine will not understand nonnumeric data we need to figure out among the features if there are different categories, then we encode those categorical data
OneHotEncoder library - for nominal data we need to do dummy encoding to assign each features as 1 otherwise machine will think the data is ordinal, and will rank/weigh according to number. 
ColumnTransformer library – to transform column
LabelEncoder library – if feature is binary we can use this, no binning needed'''

# Encoding categorical data
# Encoding the Independent Variable
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')
X = np.array(ct.fit_transform(X))
print(X)

# Encoding the Dependent Variable
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(y)
print(y)
- The remainder='passthrough' is important, otherwise it will transform only the given columns and remove the rest
- The columns or variables that we get from OneHotEncoding are called dummy variables.

'''Split dataset to test and train
model_selection library will split data set into whatever test size we put in'''

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)
print(X_train)
print(X_test)
print(y_train)
print(y_test)

'''Feature Scaling
Feature Scaling should be done after the data split as the data that’ll be coming in, will be raw data initially, that’s how the test data should be.
If we have for example: age and salary in feature variables, the scale will be of very different range from each other, 
the machine will weigh salary more than age in terms of calculation because of the Euclidean distance. 
That is why we need to scale data using either of two methods: Standardization and Normalization. 
Standardization uses standard deviation and mean, and Normalization uses min and max values. 
Normalization can be used when most of the features have Normal distribution. Otherwise pick Standardization most of the time.'''

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])
X_test[:, 3:] = sc.transform(X_test[:, 3:])
print(X_train)
print(X_test)

'''- It is important to fit transform only the X_train however the transform function should be the same for test set.
- Dummy variables do not need be scaled.'''
